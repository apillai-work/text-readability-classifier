{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9515c1",
   "metadata": {},
   "source": [
    "#  Using Naive Bayes for Text Readability Classification\n",
    "\n",
    "Code cells have been individually cited via comments wherever third-party code has been referred to or implemented, and a citation list has been added at the bottom of this notebook in Harvard style referencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1ff0e",
   "metadata": {},
   "source": [
    "### Project Overview:\n",
    "\n",
    "The purpose of this project is to create a text readability classifier (inspired by the flesch kincaid readability tests) that determines whether a piece of text is easy or hard to read. I shall be making use of english textbooks from South-East Asian / Middle Eastern areas as datasets. Since most readability classifiers use data from the United Kingdom / United States in their model, I thought it would be interesting to approach this problem using data from non-western regions to see if they could predict readability scores accurately for english phrases across the world. After building the classifier, I shall test it on speech / interview transcripts of various politicians as a use case to get a bit more insight into their speaking styles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a44b8a",
   "metadata": {},
   "source": [
    "### Project Aim:\n",
    "\n",
    "1) To construct a model that allows writers to have more control over their writing, so that they could structure their work according to their intended audience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08450576",
   "metadata": {},
   "source": [
    "### Installing and Importing the Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09046ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import textstat\n",
    "import re\n",
    "from cleantext import clean\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ccef4",
   "metadata": {},
   "source": [
    "### Selection of Data:\n",
    "\n",
    "For this project, I'm using English textbooks of varying grades from different countries. I found all of them on [Library Genesis](https://www.libgen.is/) and since they were PDF files, I then converted them to text files using [Zamzar File Converter](https://www.zamzar.com/). I initially tried using python modules for this task like PDF Miner and PyPDF, but kept running into errors as most of the code I found on StackOverflow was not suitable with the latest version of Python. \n",
    "\n",
    "For this notebook, I have used **fourth and twelfth grade english textbooks from Afghanistan by the country's Ministry of Education (2011 Edition)**, which can be found [here](https://libgen.is/search.php?req=afghanistan+english&lg_topic=libgen&open=0&view=simple&res=25&phrase=1&column=def). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1b588",
   "metadata": {},
   "source": [
    "### Preprocessing the Data:\n",
    "\n",
    "I've used Regex and the Clean-Text Library to prepare the data before the classification task. I defined a 'read_and_clean' function to read any given text file and clean the data in it, whilst replacing the line-breaks according to every condition (as described in the comments) as the text files for this task aren't following a particular pattern with grammar since it was converted from an image-heavy PDF. After that, I'm splitting the sentence after every full stop ('.') and avoiding any sentences with less than two words as it won't be of much use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33271c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(text):\n",
    "    text = re.sub(r\"#\\S+\", \" \", text) #remove hashtags\n",
    "    text = re.sub(r\"https*\\S+\", \" \", text) #remove URLs\n",
    "    text = re.sub(r'\\w*\\d+\\w*', '', text) #remove numbers\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\n\\?!\\.]', ' ', text) #remove special characters\n",
    "    text = text.strip(\" \")\n",
    "    text = text.strip(\".\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10646e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file and clean it.\n",
    "def read_and_clean(file_name):\n",
    "# read the file\n",
    "    fs = open(file_name, 'r') \n",
    "    book1 = fs.read()\n",
    "# convert it to . if 2 or more line breaks are together\n",
    "    book1 = re.sub(r\"\\n{2,}\",\". \", book1)\n",
    "# convert it to . if 2 or more spaces are together\n",
    "    book1 = re.sub(r\"\\s{2,}\",\". \", book1)\n",
    "# convert a single line break to space if it is followed by a small letter\n",
    "    book1 = re.sub(r\"\\n{1}(?=\\s[a-z])\",\" \", book1)\n",
    "# convert a single line break to space if it is followed by a space and small letter\n",
    "    book1 = re.sub(r\"\\n{1}(?=[a-z])\",\" \", book1)\n",
    "# convert all remaining line breaks to .\n",
    "    book1 = re.sub(r\"\\n\",\". \", book1)\n",
    "    total = []\n",
    "    \n",
    "    clean(book1,\n",
    "        no_urls=True) # https://pypi.org/project/clean-text/\n",
    "    \n",
    "# split the sentence after every '.'\n",
    "    for i in book1.split(\". \"):\n",
    "# clean it using the above function\n",
    "        clean_text = remove(i)\n",
    "# convert the sentence to a list of words and check the length. if it is greater then 2, then consider it a sentence\n",
    "        if len(word_tokenize(clean_text)) >2:\n",
    "            total.append(clean_text)\n",
    "# return the final list\n",
    "    return total    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835647ae",
   "metadata": {},
   "source": [
    "### Labelling the Data and Calling the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba85a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the grade one file\n",
    "grade_one_sentence = read_and_clean(\"../data/gradefourafghan.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c03d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 0\n",
    "new_examples1 = []\n",
    "for i in grade_one_sentence:  \n",
    "    if len(word_tokenize(i)) >2:\n",
    "        new_examples1 = new_examples1 + [[i, label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80aac81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples1 = new_examples1[16:] # slicing the few sentences in the beginning to remove the contents page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2baccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the grade ten file\n",
    "grade_ten_sentence = read_and_clean(\"../data/gradetwelveafghan.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0f17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 1\n",
    "new_examples2 = []\n",
    "for i in grade_ten_sentence:  \n",
    "    if len(word_tokenize(i))>2:\n",
    "        new_examples2 = new_examples2 + [[i, label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a205dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_examples2 = new_examples2[17:] # slicing the few sentences in the beginning to remove the contents page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37895982",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples2 = new_examples2[500:] # slicing further to avoid overfitting due to data imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482a33b",
   "metadata": {},
   "source": [
    "### Checking for Data Imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ec8f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_examples1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8005be3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2601"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_examples2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35bb45e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3524"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_examples1)+len(new_examples2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477fa236",
   "metadata": {},
   "source": [
    "### Checking the Readability Scores using the Textstat Library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ffdbba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = open('../data/gradefourafghan.txt', 'r') \n",
    "bookone = fs.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f061a663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pypi.org/project/textstat/\n",
    "bookonescore = round(textstat.flesch_kincaid_grade(bookone))\n",
    "bookonescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b258aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = open('../data/gradetwelveafghan.txt', 'r') \n",
    "booktwo = fs.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea5ca125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pypi.org/project/textstat/\n",
    "booktwoscore = round(textstat.flesch_kincaid_grade(booktwo))\n",
    "booktwoscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70458d8b",
   "metadata": {},
   "source": [
    "### Organising the Labelled Data together using a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cab6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(columns = [\"text\", \"label\"])  \n",
    "dataset = dataset.append(pd.DataFrame(new_examples2+new_examples1, columns = [\"text\", \"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ffee5",
   "metadata": {},
   "source": [
    "### TFIDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55fccd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashwathi/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# https://git.arts.ac.uk/lmccallum/nlp-21-22/blob/master/NLP%20Week%204.1%20-%20Classification%20Task.ipynb\n",
    "# Tokeniser!\n",
    "vectoriser = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "#Fitting the model\n",
    "tfidf_model = vectoriser.fit(dataset[\"text\"])\n",
    "#Getting vectors for everything\n",
    "tfidf = tfidf_model.transform(dataset[\"text\"]).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae14a93",
   "metadata": {},
   "source": [
    "### Train Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0419bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 353 points : 13, 96.317\n"
     ]
    }
   ],
   "source": [
    "# https://git.arts.ac.uk/lmccallum/nlp-21-22/blob/master/NLP%20Week%204.1%20-%20Classification%20Task.ipynb\n",
    "features = tfidf\n",
    "labels = np.array(dataset[\"label\"], dtype = int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "#Training the model\n",
    "model = gnb.fit(X_train, y_train);\n",
    "#Test \n",
    "y_pred = model.predict(X_test)\n",
    "num_incorrect = (y_test != y_pred).sum()\n",
    "total = y_test.shape[0]\n",
    "acc = (total - num_incorrect) / total * 100\n",
    "print(\"Number of mislabeled points out of a total %d points : %d, %0.3f\" % (total, num_incorrect, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e72e3",
   "metadata": {},
   "source": [
    "### Adding in Some New Strings to Test the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccd1a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_text = [\"I am under siege. I agree with you on that.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b665cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://git.arts.ac.uk/lmccallum/nlp-21-22/blob/master/NLP%20Week%204.1%20-%20Classification%20Task.ipynb\n",
    "# turning new text into vectors\n",
    "new_tfidf = tfidf_model.transform(my_new_text).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fa48173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am under siege. I agree with you on that.  -> class: 1\n"
     ]
    }
   ],
   "source": [
    "# https://git.arts.ac.uk/lmccallum/nlp-21-22/blob/master/NLP%20Week%204.1%20-%20Classification%20Task.ipynb\n",
    "# trying new data \n",
    "your_new_data = new_tfidf\n",
    "y_pred = model.predict(your_new_data)\n",
    "# looking at predictions\n",
    "for i, t in enumerate(my_new_text):\n",
    "    print(t,\" -> class:\", y_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82f6b9",
   "metadata": {},
   "source": [
    "### Cross-checking it with the Textstat Library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "521e8c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pypi.org/project/textstat/\n",
    "trump = round(textstat.flesch_kincaid_grade(\"I am under siege. I agree with you on that.\"))\n",
    "trump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a474c",
   "metadata": {},
   "source": [
    "#### `(All observations and findings shall be included in the critical essay).`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbbc95",
   "metadata": {},
   "source": [
    "### Citation List:    \n",
    "\n",
    "#### Websites:\n",
    "\n",
    "1) Davis, A., 2021. The fundamentals of programming - Python Video Tutorial | LinkedIn Learning, formerly Lynda.com. [online] LinkedIn. Available at: <https://www.linkedin.com/learning/programming-foundations-fundamentals-3/the-fundamentals-of-programming?autoAdvance=true&autoSkip=false&autoplay=true&resume=true&u=57077561> [Accessed 24 October 2021].\n",
    "\n",
    "2) Dib, F., 2021. regex101: build, test, and debug regex. [online] regex101. Available at: <https://regex101.com/> [Accessed 4 December 2021].\n",
    "\n",
    "3) Libgen.is. 2021. Library Genesis. [online] Available at: <https://www.libgen.is/> [Accessed 4 November 2021].\n",
    "\n",
    "4) McCallum, L., 2021. NLP Week 4.1 - Classification Task Notebook. [online] GitHub. Available at: <https://git.arts.ac.uk/lmccallum/nlp-21-22/blob/master/NLP%20Week%204.1%20-%20Classification%20Task.ipynb> [Accessed 16 November 2021].\n",
    "\n",
    "5) Nisbet, J., 2021. Python for students - Python Video Tutorial | LinkedIn Learning, formerly Lynda.com. [online] LinkedIn. Available at: <https://www.linkedin.com/learning/python-for-students/python-for-students?autoAdvance=true&autoSkip=false&autoplay=true&resume=false&u=57077561> [Accessed 18 October 2021].\n",
    "\n",
    "6) Portilla, J., 2021. Natural Language Processing with Python. [online] Udemy. Available at: <https://www.udemy.com/course/nlp-natural-language-processing-with-python/?ranMID=39197&ranEAID=JVFxdTr9V80&ranSiteID=JVFxdTr9V80-gIa4CDf8o_3HXX8ZIg_F1g&LSNPUBID=JVFxdTr9V80&utm_source=aff-campaign&utm_medium=udemyads> [Accessed 27 October 2021].\n",
    "\n",
    "7) Python, R., 2021. Practical Text Classification With Python and Keras – Real Python. [online] Realpython.com. Available at: <https://realpython.com/python-keras-text-classification/> [Accessed 2 December 2021].\n",
    "\n",
    "8) Rose, D., 2021. Artificial Intelligence Foundations: Neural Networks Video Tutorial | LinkedIn Learning, formerly Lynda.com. [online] LinkedIn. Available at: <https://www.linkedin.com/learning/artificial-intelligence-foundations-neural-networks/welcome?autoAdvance=true&autoSkip=false&autoplay=true&resume=true&u=57077561> [Accessed 6 December 2021].\n",
    "\n",
    "9) PyPI. 2021. clean-text. [online] Available at: <https://pypi.org/project/clean-text/> [Accessed 14 November 2021].\n",
    "\n",
    "10) PyPI. 2021. textstat. [online] Available at: <https://pypi.org/project/textstat/> [Accessed 15 November 2021].\n",
    "\n",
    "11) Stack Abuse. 2021. Using Regex for Text Manipulation in Python. [online] Available at: <https://stackabuse.com/using-regex-for-text-manipulation-in-python/> [Accessed 16 November 2021].\n",
    "\n",
    "12) Zamzar.com. 2021. Zamzar - video converter, audio converter, image converter, eBook converter. [online] Available at: <https://www.zamzar.com/> [Accessed 7 November 2021].\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
